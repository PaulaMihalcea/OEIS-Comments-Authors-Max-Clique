{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OEIS Analysis: Graph of Authors from Comments\n",
    "## Author: Paula Mihalcea\n",
    "#### Universit√† degli Studi di Firenze"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Requirements](#requirements)\n",
    "3. [Sequence files](#sequence-files)\n",
    "4. [Author parsing](#author-parsing)\n",
    "    - [Regular expressions](#regular-expressions)\n",
    "    - [Parsing function](#parsing-function)\n",
    "5. [Graph building](#graph-building)\n",
    "6. [Maximal cliques](#maximal-cliques)\n",
    "    - [Definitions](#definitions)\n",
    "    - [Greedy algorithm](#greedy-algorithm)\n",
    "    - [Bron-Kerbosch algorithm](#bron-kerbosch-algorithm)\n",
    "        - [Bron-Kerbosch classic](#bron-kerbosch-classic)\n",
    "\t    - [Bron-Kerbosch with Tomita pivoting](#bron-kerbosch-with-tomita-pivoting)\n",
    "        - [Bron-Kerbosch with degeneracy ordering](#bron-kerbosch-with-degeneracy-ordering)\n",
    "\t    - [Complexity](#complexity)\n",
    "    - [Maximum clique](#finding-the-maximum-clique)\n",
    "7. [Conclusions](#conclusions)\n",
    "8. [Testing](#testing)\n",
    "9. [License](#license)\n",
    "10. [References](#references)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "**[OEIS](https://oeis.org/)** is the online encyclopaedia of **integer sequences**. It lists thousands of number sequences in lexicographic order, such as the [prime numbers](http://oeis.org/A000040) or the [Fibonacci sequence](http://oeis.org/A000045), easing the work of countless researchers since 1964, its foundation year.\n",
    "\n",
    "The OEIS is made of a series of **JSON files**, one for each integer sequence. Given their regular, human-readable format, these files can be easily manipulated in order to further analyze them. Indeed, each page of the OEIS not only lists the integers of the corresponding sequence, but also a series of information such as formulas, references, links and comments.\n",
    "\n",
    "This work aims to create, step-by-step, a **[Python 3](https://www.python.org/)** script capable of loading these files and parsing their content in order to build a **graph** where:\n",
    "- **nodes** represent all unique **authors** that can be found in each comment of every sequence, and\n",
    "- **edges** link two authors who have **commented the same sequence**.\n",
    "\n",
    "Three main algorithms are then implemented in order to find:\n",
    "1. a **maximal clique**;\n",
    "2. a list of **all maximal cliques**;\n",
    "3. the **maximum clique**.\n",
    "\n",
    "The library of choice for creating the graph is **[NetworkX](https://networkx.org/)**, a fast Python module for the creation, manipulation, and study of the structure of complex networks. Other libraries such as [itertools](https://docs.python.org/3/library/itertools.html), [NumPy](https://numpy.org/), [os](https://docs.python.org/3/library/os.html) and [random](https://docs.python.org/3/library/random.html) are also used for efficiency purposes, as they provide highly optimized functions. The complete list of packages can be found in the [requirements section](#requirements)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements\n",
    "\n",
    "Before starting, a series of packages must be installed for the subsequent code to be executable. The simplest way is to use [`pip`](https://pypi.org/project/pip/), a package manager for Python callable from the system terminal.\n",
    "\n",
    "The commands needed for this operation are listed in the following cell; the Jupyter magic function [`%%cmd`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-bash) (`%%bash` for Unix users) at the beginning allows to use it as a terminal. Make sure to follow the recommended install order, as it helps avoiding errors which can sometimes be generated by different versions of the packages.\n",
    "\n",
    "Note: the `argparse` package is only needed for the execution of the `mihalcea.py` script, and are not necessary for the current Jupyter notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%cmd\n",
    "\n",
    "pip install numpy\n",
    "pip install networkx\n",
    "pip install tqdm\n",
    "pip install argparse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The freshly installed modules can be now used by simply importing them, along with other native Python packages:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse # Only needed for the mihalcea.py script\n",
    "import itertools as its\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import timeit\n",
    "import tqdm\n",
    "import warnings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequence files\n",
    "\n",
    "Having installed the required packages, we can now proceed with analyzing the files.\n",
    "\n",
    "The raw OEIS sequence files can be found in [`data/sequences`](./data/sequences/). We can start by writing a function capable of opening one of them using the [JSON package](https://docs.python.org/3/library/json.html) available in Python, and use it to load a file's content as a Python [dict](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict), then print it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_json(file_path, print_result=False):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            raw_data = json.load(file)\n",
    "            if print_result:\n",
    "                print('File ' + file_path.split('/')[-1] + ' contents:')\n",
    "                print()\n",
    "                print(json.dumps(raw_data, indent=True))\n",
    "                print()\n",
    "                print(\n",
    "                    'The \\'json\\' Python module returns a dictionary, which can be confirmed by invoking the \\'type\\' function on the loaded data: ' + str(\n",
    "                        type(raw_data)) + '.')\n",
    "                print('This dictionary\\'s keys are: ' + str(raw_data.keys()).replace('dict_keys([', '').replace('])',\n",
    "                                                                                                                '') + '.')\n",
    "            return raw_data\n",
    "    except OSError:\n",
    "        print('Could not open file: {}, exiting program.'.format(file_path.split('/')[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this function correctly **handles input/output errors**, and can be used to **return a file's content** as a Python **dictionary** even without printing it, by either omitting the `print_result` argument or setting it to `False` - a feature which will soon come in handy.\n",
    "\n",
    "We can thus view the first JSON file and its keys:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load sample sequence file\n",
    "print('\\n' + 'Printing sample OEIS JSON file...')\n",
    "\n",
    "file = load_json('data/sequences/A000001.json', print_result=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned before, each sequence file contains additional information, specifically:\n",
    "- a simple `greeting`;\n",
    "- a `query`, containing the sequence's ID;\n",
    "- `count`;\n",
    "- `start`;\n",
    "- `results`, which contains a list with another dictionary as its first element.\n",
    "\n",
    "It can be seen from this file's content that the most relevant information is actually found in the **`results` sub-dictionary**, which can be easily accessed with:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print 'results' section of the sample sequence\n",
    "print('\\n' + 'Printing sample file \"results\" section...' + '\\n')\n",
    "\n",
    "results = file.get('results')\n",
    "if results:\n",
    "    print(json.dumps(results[0], indent=True))\n",
    "else:\n",
    "    print('No \"results\" section found.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, there are many different keys, among which we can find the one which is relevant to this project: the `comment` key containing a list of **comments** with their **authors**:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print 'comment' subsection of the sample sequence\n",
    "print('\\n' + 'Printing sample file \"comment\" subsection...' + '\\n')\n",
    "\n",
    "comment_list = results[0].get('comment')\n",
    "if comment_list:\n",
    "    print(json.dumps(comment_list, indent=True))\n",
    "else:\n",
    "    print('No \"comments\" subsection found.' + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Author parsing\n",
    "\n",
    "Now that we know where to find the authors' names, we can proceed with building a function to parse all of them from a given file.\n",
    "\n",
    "### Regular expressions\n",
    "The most efficient way of doing this is to use a **regular expression** (also known as *regex*), a set of characters specifying a *search pattern*.\n",
    "\n",
    "We must first identify the ways in which the names have been written; by analyzing some comments, **six main patterns** have been identified, along with the **four regular expressions** needed to match them:\n",
    "1. *\"\\_Name Surname\\_\"* `(?<=_)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=_)`\n",
    "2. *\"\\[Name Surname\\]\"* and *\"\\[Surnamea, Surnameb\\]\"* `(?<=\\[)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=\\])`\n",
    "3. *\"- Name, Surname ( \"* and *\"\\- Name Surname, \"* `(?<=- )[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?= \\(|, )`\n",
    "4. *\"(Name Surname,\"* `(?<=\\()[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=,)`\n",
    "\n",
    "In spite of their apparent complexity, the meaning of these patterns is quite simple and be easily debugged with tools like [Regex101](https://regex101.com/). Each of them matches only strings that:\n",
    "- begin with certain characters `_`, `[`, `- `, `(`,\n",
    "    - followed by a capital letter `[A-Z]`,\n",
    "        - not followed by another capital letter `(?!=[A-Z])`,\n",
    "    - followed by at least any two characters `{2,}?`,\n",
    "        - at the condition that none of them belong to a list of forbidden symbols `[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]` (where `^` is as a negation operator),\n",
    "- end with certain characters `_`, `]`, `(` or `, `, `,`.\n",
    "\n",
    "`(?>=)` and `(?=)` indicate that the matched strings should be preceded or followed (respectively) by the character(s) to the right of the `=` symbol.\n",
    "\n",
    "Escaping certain characters distinguishes them from a regex special symbol (e.g. `\\(\\)` matches the string *()*, while `()` is an empty regex group); whitespaces are simply represented by, well, a whitespace (` `).\n",
    "\n",
    "By combining these four expressions with the OR character (`|`) we can create the following regular expression to match all patterns at once in Python:\n",
    "\n",
    "`(?<=_)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=_)|(?<=\\[)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=\\])|(?<=- )[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?= \\(|, )|(?<=\\()[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=,)`\n",
    "\n",
    "#### Completeness\n",
    "It should be noted that these expressions **do not find all the authors** present in the comments because they are not written consistently across all sequences. One might argue that it would be sufficient finding all patterns used in order to get all the names; while this would be a good, if not really feasible solution (we do not know how many they are), the problem remains because certain patterns also match formulas and other unrelated data, making them unusable for retrieving only names.\n",
    "\n",
    "The definitive solution would be to either manually get the names, or to allow the matching of extraneous data in order to remove it later from the list of names; this would take too long, though, and goes beyond the purpose of this project.\n",
    "\n",
    "### Parsing function\n",
    "The parsing function gets the `dict` **raw data** read by the JSON library in input and returns a **set of all author names** present in the comments of the loaded file (or `None` if there are none).\n",
    "\n",
    "Basically, after preparing the regex pattern (`re.compile()`), for each `comment` in a non-empty `comment_list` the function gets a list of the authors' names using Python's [`re`](https://docs.python.org/3/library/re.html) package for regular expressions, and uses it to update the set of unique authors called `authors` (which contains all names found in the file). The list comprehension in the `update` method is needed to flatten the many lists of lists returned by `re.findall()`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_authors_from_comments(raw_data):\n",
    "    # Regex pattern\n",
    "    common_pattern = r'[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?'\n",
    "    pattern_list = [('(?<=_)', '(?=_)'), ('(?<=\\[)', '(?=\\])'), ('(?<=- )', '(?= \\(|, )'), ('(?<=\\()', '(?=,)')]\n",
    "    pattern = re.compile('|'.join([start + common_pattern + end for start, end in pattern_list]))\n",
    "\n",
    "    # Comment parsing\n",
    "    comment_list = raw_data.get('results')[0].get('comment')\n",
    "    if comment_list:\n",
    "        authors = set()\n",
    "        for comment in comment_list:\n",
    "            authors.update([n for names in re.findall(pattern, comment) for n in names.split(', ')])\n",
    "        return authors\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some observations:\n",
    "- the regex pattern is initially split into its **subpatterns** for better readability and to avoid repetitions;\n",
    "- this pattern has been accurately written so as to **not return empty matches**, normally generated by *capturing groups* (groups of characters between round parentheses) and for which additional `if`s would have been needed, resulting in a more complicated list comprehension;\n",
    "- **some sequences do not contain comments**, hence the check on `comment_list`;\n",
    "- a **set** has been chosen for the `authors_set` variable in order to **exclude duplicate names**, since the data needed for the project only concerns the presence or absence of a given author in the comments of a sequence, not all his/her instances. Python's [`set`](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset) data structure allows to store items in a hash table, without duplicating them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph building\n",
    "\n",
    "We can now proceed by parsing the authors from all OEIS sequences in the `data/sequences` directory and build their graph using the NetworkX library, eventually saving it to disk to avoid loading every time all the JSON files.\n",
    "\n",
    "Considering that each **node** of the graph should contain the **name of a single author** (without duplicates), we only need to:\n",
    "1. add each author of each sequence as a node;\n",
    "2. add edges between all pairs of authors which have commented the same sequence.\n",
    "\n",
    "By repeating this procedure for every file in the `data/sequences` directory we get a graph of all authors, where people who have commented the same sequence are connected by an edge.\n",
    "\n",
    "The creation of such a graph is quite simple with the NetworkX library, since we only need to:\n",
    " - parse each sequence file;\n",
    " - extract its authors;\n",
    " - add them as nodes;\n",
    " - create a list of all possible pairs of authors in each sequence;\n",
    " - add an edge for each pair.\n",
    "\n",
    "Since the first two operations have been already implemented in the previous steps (see the `parse_authors_from_comments()` function), the other two are as simple as two lines of code, knowing that **NetworkX does not complain when adding existing nodes or edges**: we do not need to check every time if a given author has already been inserted or if a certain edge already exists, because the library will *not* duplicate them\\[[1](https://networkx.org/documentation/stable/reference/classes/graph.html)\\]. In fact, we could even skip the `add_nodes_from()` function, since NetworkX automatically inserts non-existing nodes when adding edges connecting them (which is why it has been commented in the code below).\n",
    "\n",
    "The best way to compute all author pairs for each sequence is given by the [itertools](https://docs.python.org/3/library/itertools.html) library, which implements efficient looping.\n",
    "\n",
    "Some notes about the `build_graph_from_directory()` function:\n",
    "- all it needs as input arguments is the **path** of the directory containing the JSON files and a **boolean flag** to specify if the resulting graph should also be saved to disk (instead of simply returned) - along with a name for the newly created JSON graph file, eventually (otherwise `comments_authors_graph.json` is applied by default);\n",
    "- it begins with checking the correctness of the JSON files path and creating the necessary variables, among which:\n",
    "    - a list of all files in the given directory (using [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir));\n",
    "    - an empty NetworkX graph `g`;\n",
    "    - a [tqdm](https://tqdm.github.io/) progress bar, only needed to visualize the overall progress of the parsing process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_graph_from_directory(dir_path, save=False, filename='comments_authors_graph'):\n",
    "    # Get file list\n",
    "    if dir_path[-1] != '/':\n",
    "        dir_path += '/'\n",
    "    file_list = [json_file for json_file in os.listdir(dir_path) if json_file.endswith('.json')]\n",
    "\n",
    "    # Prepare variables\n",
    "    g = nx.Graph()\n",
    "    progress_bar = tqdm.tqdm(total=len(file_list))\n",
    "\n",
    "    # Parse all JSON files\n",
    "    for f in file_list:\n",
    "        progress_bar.set_description('Parsing file {}'.format(f))\n",
    "        file_path = dir_path + f\n",
    "        raw_data = load_json(file_path)\n",
    "\n",
    "        authors = parse_authors_from_comments(raw_data)\n",
    "        if authors:\n",
    "            # g.add_nodes_from(authors)\n",
    "            g.add_edges_from(list(its.combinations(authors, 2)))\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Save graph\n",
    "    if save:\n",
    "        try:\n",
    "            with open(dir_path.split('/')[0] + '/' + filename + '.json', 'w') as out_file:\n",
    "                json.dump(nx.readwrite.json_graph.node_link_data(g), out_file)\n",
    "        except OSError:\n",
    "            print('Could not save file: {}, exiting program.'.format(filename + '.json'))\n",
    "\n",
    "    return g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, assuming that the graph has already been built and saved to disk, it can be loaded from an existing JSON file with the `load_json_graph()` function, which simply takes the JSON graph's path as input:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_json_graph(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            raw_data = json.load(file)\n",
    "            return nx.readwrite.json_graph.node_link_graph(raw_data)\n",
    "    except OSError:\n",
    "        print('Could not open file: {}, exiting program.'.format(file_path.split('/')[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph can thus be created by running:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graph creation (either from raw data or existing JSON graph)\n",
    "build_graph = False  # Set to 'True' in order to build graph from raw data\n",
    "\n",
    "if build_graph:  # Build graph and save to file\n",
    "    print('\\n' + 'Building graph g, where:')\n",
    "    print('- nodes represent all unique authors that can be found in each comment of every sequence;')\n",
    "    print('- edges link two authors who have commented the same sequence...')\n",
    "\n",
    "    g = build_graph_from_directory('data/sequences', save=True)\n",
    "else:  # Load graph from disk\n",
    "    print('\\n' + 'Loading graph g from \"data/comments_authors_graph.json\", where:')\n",
    "    print('- nodes represent all unique authors that can be found in each comment of every sequence;')\n",
    "    print('- edges link two authors who have commented the same sequence.')\n",
    "\n",
    "    g = load_json_graph('data/comments_authors_graph.json')\n",
    "\n",
    "print('\\n' + 'Graph g has {} nodes and {} edges.'.format(len(g.nodes), len(g.edges)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All variable names are lowercase with words separated by undescores in order to be compliant with the Python Enhancement Proposals 8 (PEP 8) style guide\\[[2](https://www.python.org/dev/peps/pep-0008/#function-and-variable-names)\\]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximal cliques\n",
    "As stated in the introduction, our goal for this project is to explore the problem of **finding maximal cliques** in a graph by building three algorithms to find:\n",
    "1. a maximal clique;\n",
    "2. a list of all maximal cliques;\n",
    "3. the maximum clique.\n",
    "\n",
    "Before proceeding with the actual Python code, we shall provide first some useful definitions and theoretical notions .\n",
    "\n",
    "### Definitions\n",
    "Let $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ be an undirected graph where $\\mathcal{V}$ is the set of all nodes and $\\mathcal{E}$ the set of all edges.\n",
    "\n",
    "> A **clique** of $\\mathcal{G}$ is a **complete subgraph**, or a simple undirected graph in which each pair of vertices is connected by an edge\\[[3](https://mathworld.wolfram.com/Clique.html)\\]\\[[4](https://mathworld.wolfram.com/CompleteGraph.html)\\].\n",
    "\n",
    "> A **maximal clique** is a clique that cannot be extended by including one more adjacent vertex, meaning it is not a subset of a larger clique.\n",
    "\n",
    "> The **maximum clique** in a graph (i.e. the clique of largest size) is always maximal, while the converse does not hold\\[[5](https://mathworld.wolfram.com/MaximalClique.html)\\]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div><img src=\"img/ex_cliques.png\" width=\"300\"/></div>\n",
    "\n",
    "<center>Figure 1: The _maximal cliques_ of this graph are {1, 2, 5, 6}, {2, 3, 5} and {3, 4, 5}. Among these, the _maximum clique_ is {1, 2, 5, 6}.</center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Finding one maximal clique:\n",
    "### Greedy algorithm\n",
    "The easiest way to find **one arbitrary maximal clique** is to run on our graph $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ a **greedy algorithm**, which makes the local optimal choice at each step:\n",
    "\n",
    "```\n",
    "Greedy-Maximal-Clique(ùí¢):\n",
    "    init: ùë† ‚àà ùí± starting node\n",
    "          ùíû ‚äÇ ‚Ñ∞, ùíû = {ùë†} the set of nodes in the maximal clique\n",
    "\n",
    "    for each ùë£ ‚àà ùí± ‚àñ {ùë†}:\n",
    "        if ‚àÉ (ùë£, ùë¢) ‚àà ‚Ñ∞   ‚àÄ ùë¢ ‚àà ùíû:\n",
    "            ùíû = ùíû ‚à™ {ùë£}\n",
    "\n",
    "    return ùíû\n",
    "```\n",
    "\n",
    "The algorithm simply keeps adding nodes connected to all of the nodes already present in the current clique $\\mathcal{C}$ until no other node is found. $\\mathcal{C}$ can be initialised to contain any node $s$ in the graph.\n",
    "\n",
    "We can devise a **more refined version** which does not test _all_ nodes in the graph, but only the neighbors of the nodes in the current clique, thus excluding those vertices which will surely never pass the test (e.g. nodes in different connected components), for better efficiency:\n",
    "\n",
    "```\n",
    "Greedy-Maximal-Clique-Neighbors(ùí¢):\n",
    "    init: ùë† ‚àà ùí± starting node\n",
    "          ùíû ‚äÇ ‚Ñ∞, ùíû = {ùë†} the set of nodes in the maximal clique\n",
    "\n",
    "    while ùí©(ùíû) ‚â† ‚àÖ:\n",
    "        ùë£ ‚àà ùí©(ùíû)\n",
    "        ùí©(ùíû) = ùí©(ùíû) ‚àñ {ùë£}\n",
    "        if ‚àÉ (ùë£, ùë¢) ‚àà ‚Ñ∞   ‚àÄ ùë¢ ‚àà ùíû:\n",
    "            ùíû = ùíû ‚à™ {ùë£}\n",
    "            ùí©(ùíû) = ùí©(ùíû) ‚à™ ùí©(ùë£)\n",
    "\n",
    "    return ùíû\n",
    "```\n",
    "\n",
    "The Python implementation is almost identical to this pseudocode, with the exception of a **`valid` flag** kept in order to quit the loop as soon as an edge $(v, u)$ does not exist, **to reduce the number of iterations**, and the fact that **only non-trivial maximal cliques are returned** (i.e. only those with more than 2 nodes are considered).\n",
    "\n",
    "The resulting function `find_one_maximal_clique_greedy()` takes in input a NetworkX graph `g` and, optionally, two boolean flags for:\n",
    " - choosing the greedy algorithm variant (`naive` or `neighbors`, of which the latter is default), and\n",
    " - printing the clique found (`print_result`, `False` by default).\n",
    "\n",
    "The choice to use nested functions has been made to unify the two variants in a single algorithm for finding a maximal clique, since they are simple enough and do not really need to be differentiated (the naive version is more a curiosity than an every-day solution).\n",
    "\n",
    "This function also checks whether the provided graph is a NetworkX undirected graph, and if it is empty or not (and raises an exception, accordingly)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_one_maximal_clique_greedy(g, variant='neighbors', print_result=False):\n",
    "    # Check that g is a NetworkX graph\n",
    "    if not isinstance(g, nx.classes.graph.Graph):\n",
    "        raise nx.NetworkXError('The provided graph is not a valid NetworkX undirected graph.')\n",
    "\n",
    "    if g.nodes:\n",
    "        # Naive variant (all nodes)\n",
    "        if variant == 'naive':\n",
    "            # Initialization\n",
    "            vertices = list(g.nodes)\n",
    "            s = random.choice(vertices)\n",
    "            vertices.remove(s)\n",
    "            clique = {s}\n",
    "\n",
    "            # Greedy algorithm\n",
    "            for v in vertices:\n",
    "                valid = True\n",
    "                for u in clique:\n",
    "                    if not g.has_edge(v, u):\n",
    "                        valid = False\n",
    "                        break\n",
    "                if valid:\n",
    "                    clique.add(v)\n",
    "        # Neighbors variant\n",
    "        else:\n",
    "            # Wrong argument warning\n",
    "            if variant != 'neighbors':\n",
    "                warnings.warn('Invalid algorithm variant (\\'{}\\'). Using greedy algorithm restricted to neighbors as default.'.format(variant))\n",
    "\n",
    "            # Initialization\n",
    "            s = random.choice(list(g.nodes))\n",
    "            neighbors = list(g.neighbors(s))\n",
    "            clique = {s}\n",
    "\n",
    "            # Greedy algorithm\n",
    "            while neighbors:\n",
    "                v = neighbors.pop()\n",
    "                valid = True\n",
    "\n",
    "                for u in clique:\n",
    "                    if not g.has_edge(v, u):\n",
    "                        valid = False\n",
    "                        break\n",
    "                if valid:\n",
    "                    clique.add(v)\n",
    "                    neighbors.extend(list(g.neighbors(v)))\n",
    "\n",
    "        # Result & printing\n",
    "        if len(clique) > 2:\n",
    "            if print_result:\n",
    "                print(clique)\n",
    "            return clique\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        raise nx.NetworkXPointlessConcept('The provided graph is empty.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Greedy algorithm results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find and print one maximal clique\n",
    "find_one_maximal_clique_greedy(g, print_result=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div><img src=\"img/ex_greedy.png\" width=\"250\"/></div>\n",
    "\n",
    "<center>Figure 2: Application of the greedy algorithm to a graph with 4 nodes.</center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Finding all maximal cliques:\n",
    "### Bron-Kerbosch algorithm\n",
    "In order to find all maximal cliques in our graph we can proceed by implementing the **Bron-Kerbosch algorithm**\\[[6](https://dl.acm.org/doi/10.1145/362342.362367)\\], designed by its Dutch namesakes in 1973 and still widely used nowadays, either in its classic form or in one of its more efficient variants.\n",
    "\n",
    "#### Bron-Kerbosch classic\n",
    "Given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, three sets of nodes, $R$, $P$ and $X$, play an important role in the algorithm:\n",
    "- $R$ is the set to be **extended or shrunk** by a new node. Nodes that are eligible to extend it, i.e. that are connected to all the other nodes in $R$, are collected recursively in the remaining two sets;\n",
    "- $P$ is the set of **candidates**, i.e. of all nodes that will in due time serve as an extension to the present configuration of $R$;\n",
    "- $X$ is the set of all nodes that have at an earlier stage already served as an extension of the present configuration of $R$ and are now explicitly **excluded**.\n",
    "\n",
    "The core of the algorithm consists of a **recursive function** applied to the three sets, which generates all extensions of the given configuration of $R$ that can be made with the nodes in $P$ and that do not contain any of the nodes in $X$ (all extensions of $R$ containing any node in $X$ have already been generated):\n",
    "\n",
    "```\n",
    "Bron-Kerbosch(ùëÖ, ùëÉ, ùëã):\n",
    "    if ùëÉ = ‚àÖ ‚àß ùëã = ‚àÖ:\n",
    "        ùëÖ is a maximal clique\n",
    "    for ùë£ ‚àà ùëÉ:\n",
    "        Bron-Kerbosch(ùëÖ ‚à™ {ùë£}, ùëÉ ‚à© ùí©(ùë£), ùëã ‚à© ùí©(ùë£))\n",
    "        ùëÉ = ùëÉ ‚àñ {ùë£}\n",
    "        ùëã = ùëã ‚à™ {ùë£}\n",
    "```\n",
    "\n",
    "The extra labor involved in maintaining the set $X$ is motivated by the fact that a necessary condition for having created a clique is that $P$ be empty, otherwise $R$ could still be extended. This condition, however, is not sufficient, because if now $X$ is non-empty, we know from the definition of $X$ that the present configuration of $R$ has already been contained in another configuration and is therefore not maximal, so we can state that $R$ is a clique only as soon as _both $X$ and $P$_ are empty.\n",
    "\n",
    "If at some stage $X$ contains a node connected to all nodes in $P$, we can predict that further extensions (further selection of candidates) will never lead to the removal (in Step 3) of that particular node from subsequent configurations of $X$ and, therefore, not to a clique. This enables us to detect in an early stage branches of the backtracking tree that do not lead to successful endpoints.\n",
    "\n",
    "In order to find all maximal cliques of a given graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, we only need to set:\n",
    "\n",
    "- $R = \\emptyset$;\n",
    "- $P = \\mathcal{V}$;\n",
    "- $X = \\emptyset$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python implementation of this algorithm is simple, as it translates the pseudocode quite literally:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Classic Bron-Kerbosch algorithm\n",
    "def bron_kerbosch(r, p, x):\n",
    "    if not p and not x:\n",
    "        if len(r) > 2:\n",
    "            yield r\n",
    "    for v in {*p}:\n",
    "        yield from bron_kerbosch(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "        p = p - {v}\n",
    "        x.add(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div><img src=\"img/ex_bk_classic.png\" width=\"400\"/></div>\n",
    "\n",
    "<center>Figure 3: Application of the Bron-Kerbosch algorithm to a graph with 4 nodes.</center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bron-Kerbosch with Tomita pivoting\n",
    "The original Bron-Kerbosch algorithm might require large amounts of memory, as it does not avoid backtracking from useless cases where $P = \\emptyset$ and $X = \\emptyset$. These unfruitful occurences can be decreased by choosing a **pivot vertex** $u \\in P \\cup X$ in such a way that maximal cliques must contain either $u$ or a vertex in $P \\setminus \\mathcal{N}(u)$, or else the clique could be extended by $u$. In other words, only nodes in $P \\setminus \\mathcal{N}(u)$ will be candidates in each recursive call to the algorithm. A simple, effective way to choose the pivot is called the **Tomita pivoting**\\[[7](https://www.sciencedirect.com/science/article/pii/S0304397506003586)\\]:\n",
    "\n",
    "> The pivot $u \\in P \\cup X$ is the node that maximises $|P \\cap \\mathcal{N}(u)|$, i.e. the node having the most neighbors in $P$.\n",
    "\n",
    "```\n",
    "Bron-Kerbosch-Tomita-Pivoting(ùëÖ, ùëÉ, ùëã):\n",
    "    if ùëÉ = ‚àÖ ‚àß ùëã = ‚àÖ:\n",
    "        ùëÖ is a maximal clique\n",
    "    choose pivot ùë¢ ‚àà ùëÉ ‚ãÉ ùëã that maximises |ùëÉ ‚à© ùí©(ùë¢)|\n",
    "    for ùë£ ‚àà ùëÉ ‚àñ ùí©(ùë¢):\n",
    "        Bron-Kerbosch-Tomita-Pivoting(ùëÖ ‚à™ {ùë£}, ùëÉ ‚à© ùí©(ùë£), ùëã ‚à© ùí©(ùë£))\n",
    "        ùëÉ = ùëÉ ‚àñ {ùë£}\n",
    "        ùëã = ùëã ‚à™ {ùë£}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, the Python version does not differ much from the pseudocode, except for a `try...except` clause needed to handle empty sets when choosing the pivot:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bron-Kerbosch algorithm with Tomita pivoting\n",
    "def bron_kerbosch_tomita_pivot(r, p, x):\n",
    "    if not p and not x:\n",
    "        if len(r) > 2:\n",
    "            yield r\n",
    "    try:\n",
    "        u = max({(v, len({n for n in g.neighbors(v) if n in p})) for v in p | x}, key=lambda v: v[1])[0]\n",
    "        for v in p - {*g.neighbors(u)}:\n",
    "            yield from bron_kerbosch_tomita_pivot(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "            p = p - {v}\n",
    "            x.add(v)\n",
    "    except ValueError:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div><img src=\"img/ex_bk_tomita.png\" width=\"500\"/></div>\n",
    "\n",
    "<center>Figure 4: Application of the pivot Bron-Kerbosch algorithm to a graph with 4 nodes.</center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bron-Kerbosch with degeneracy ordering\n",
    "Apart from the pivoting strategy, the **order** in which the vertices of $\\mathcal{G}$ are processed by the Bron‚ÄìKerbosch algorithm is also very important. Before continuing, let us see the notion of **degeneracy**, which will help to illustrate the next approach.\n",
    "\n",
    "> The **degeneracy** of an $n$-vertex graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ is the smallest number $d$ such that every subgraph of $\\mathcal{G}$ contains a vertex of degree at most $d$.\n",
    "\n",
    "> A graph with degeneracy $d$ also has a **degeneracy ordering**, i.e. an ordering of the vertices such that each vertex has $d$ or fewer neighbors that come later in the ordering.\n",
    "\n",
    "Degeneracy, along with a degeneracy ordering, can be computed by a simple **greedy strategy** of repeatedly removing a vertex with smallest degree (and its incident edges) from the graph until it is empty:\n",
    "\n",
    "```\n",
    "Degeneracy-Ordering(ùí¢):\n",
    "    init: ùê∑ array s.t. ùê∑[ùëñ] stores the list of vertices ùë£ ‚àà ùí± of degree ùëñ\n",
    "          ùëë array containing the degeneracy ordering\n",
    "\n",
    "    while ùê∑ ‚â† ‚àÖ:\n",
    "        scan ùê∑ until the first non-empty list ùê∑[ùëñ] is found\n",
    "        move a vertex ùë¢ from ùê∑[ùëñ] to ùëë\n",
    "        for ùë£ ‚àà ùí©(ùë¢):\n",
    "            move ùë£ from ùê∑[ùëó] to ùê∑[ùëó-1], where ùëó is the degree of ùë£\n",
    "        remove ùë¢ from the graph ùí¢\n",
    "\n",
    "    return ùëë\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div><img src=\"img/ex_deg_order.png\" width=\"800\"/></div>\n",
    "\n",
    "<center>Figure 5: Degeneracy ordering algorithm applied to the graph shown in figures 2 and 3.</center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these facts, Eppstein et al.\\[[8](https://link.springer.com/chapter/10.1007%2F978-3-642-17517-6_36)\\] showed in 2010 that there exists a nearly-optimal algorithm for **enumerating all maximal cliques parametrized by degeneracy**, and that in order to achieve this result a modification of the classic Bron‚ÄìKerbosch algorithm was sufficient.\n",
    "\n",
    "They performed the outer level of recursion of the Bron‚ÄìKerbosch algorithm without pivoting, using a degeneracy ordering to order the sequence of recursive calls, and then switched at inner levels of recursion to the pivoting rule of Tomita et al.\\[[7](https://www.sciencedirect.com/science/article/pii/S0304397506003586)\\]:\n",
    "\n",
    "```\n",
    "Bron-Kerbosch-Degeneracy(ùí¢):\n",
    "    init: ùëÖ = ‚àÖ\n",
    "          ùëÉ = ùí±\n",
    "          ùëã = ‚àÖ\n",
    "          ùëë = Degeneracy-Ordering(ùí¢)\n",
    "\n",
    "    for ùë£ ‚àà ùëë:\n",
    "        Bron-Kerbosch-Tomita-Pivoting(ùëÖ ‚à™ {ùë£}, ùëÉ ‚à© ùí©(ùë£), ùëã ‚à© ùí©(ùë£))\n",
    "        ùëÉ = ùëÉ ‚àñ {ùë£}\n",
    "        ùëã = ùëã ‚à™ {ùë£}\n",
    "```\n",
    "\n",
    "Thanks to this ordering, the sets $P$ passed to each of the recursive calls will have at most $d$ elements in them, minimizing the recursive calls within each of the outer calls, while the set $X$ will consist of all earlier neighbors of $v$ (could be larger than $d$)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These two algorithms can be implemented in Python as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_degeneracy_ordering(graph):\n",
    "    # Check that g is a NetworkX graph\n",
    "    if not isinstance(graph, nx.classes.graph.Graph):\n",
    "        raise nx.NetworkXError('The provided graph is not a valid NetworkX undirected graph.')\n",
    "\n",
    "    if graph.nodes:\n",
    "        g = graph.copy()\n",
    "\n",
    "        # Create and populate lists of lists\n",
    "        max_degree = max([d for n, d in g.degree()])\n",
    "        d = [[] for deg in range(max_degree + 1)]\n",
    "        for node in g.degree():\n",
    "            d[node[1]].append(node[0])\n",
    "\n",
    "        # Degeneracy ordering\n",
    "        degeneracy_ordering = []\n",
    "        while d:\n",
    "            # Get current node u\n",
    "            u = next(i for i in d if i).pop()\n",
    "            degeneracy_ordering.append(u)\n",
    "\n",
    "            # Move neighbors of current node\n",
    "            for v in {*g.neighbors(u)}:\n",
    "                v_deg = g.degree(v)\n",
    "                d[v_deg].remove(v)\n",
    "                d[v_deg-1].append(v)\n",
    "\n",
    "            # Remove current node from graph\n",
    "            g.remove_node(u)\n",
    "\n",
    "            # Remove last list of d if empty (ensure termination of while loop)\n",
    "            if not d[len(d)-1]:\n",
    "                d.pop()\n",
    "\n",
    "        return degeneracy_ordering\n",
    "    else:\n",
    "        raise nx.NetworkXPointlessConcept('The provided graph is empty.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`get_degeneracy_ordering()` begins by checking whether the provided graph is a NetworkX undirected graph, and if it is empty or not, and raises an exception, accordingly. It then continues by making a copy of the input graph, which is necessary because the progressive removal of nodes would otherwise leave the original graph empty after applying this function to it.\n",
    "\n",
    "The function does not differ much from its pseudocode, apart from the use of a `while` loop which is terminated by progressively removing any empty list at the tail of `d` (since neighbors of the current node will always be moved to a list of lower degree, and never higher). Given the need for dynamic arrays, the simple Python list has been used instead of more efficient data structures like in the clique methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bron-Kerbosch algorithm with Tomita pivoting & degeneracy ordering\n",
    "def bron_kerbosch_degeneracy(r, p, x):\n",
    "    for v in get_degeneracy_ordering(g):\n",
    "        yield from bron_kerbosch_tomita_pivot(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "        p = p - {v}\n",
    "        x.add(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Complexity\n",
    "The **worst-case analysis** for the Bron-Kerbosch algorithm is $O(3^{\\frac{n}{3}})$ running time. It is optimal as a function of $n$, since there are at most $3^{\\frac{n}{3}}$ maximal cliques in an $n$-vertex graph\\[[9](https://link.springer.com/article/10.1007/BF02760024)\\]. It also has the nice property that it generates **all and only maximal cliques without duplication**.\n",
    "\n",
    "**Eppstein et al.'s variant** instead runs in time $O(dn3^{\\frac{d}{3}})$, and the degeneracy $d$ is expected to be low in many real-world applications. The time needed to obtain the degeneracy ordering is irrelevant, as it runs linear to the number of vertices $n$ and edges $m$ of the graph, i.e. $O(n+m)$.\n",
    "\n",
    "Eppstein et al.'s results originate from the observation that given a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ with degeneracy $d$:\n",
    "- $\\mathcal{G}$ has at most $d(n-\\frac{d+1}{2})$ edges;\n",
    "- the maximum clique size can be at most $d+1$, for any larger clique would form a subgraph in which all vertices have degree higher than $d$;\n",
    "- if $d$ is a multiple of $3$ and $n \\geq d+3$, then the largest possible number of maximal cliques is $(n-d)3^{\\frac{d}{3}}$.\n",
    "\n",
    "**Real-world graphs** are actually quite **sparse**, making the **degeneracy** version of the Bron-Kerbosch algorithm an **excellent choice**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Wrapper function for the Bron-Kerbosch algorithm\n",
    "\n",
    "The actual Python implementation of the presented algorithms consist of a single method `find_all_maximal_cliques_bk()` which defines three nested functions, one for each Bron-Kerbosch variant, and which takes in input only a NetworkX graph `g` and, optionally, two boolean flags for:\n",
    " - choosing the Bron-Kerbosch variant (`classic`, `tomita` and `degeneracy`, of which the latter is default), and\n",
    " - printing the cliques found (`print_result`, `False` by default).\n",
    "\n",
    "The choice to use nested functions has been made to avoid repeating, for every variant, the definition of the sets $R$, $P$ and $X$ used in all three variants of the algorithm, as well as the checks on the input graph. This way the algorithms work correctly without compromising their legibility with language-specific code, resulting in an almost literal implementation of the pseudocode previously provided.\n",
    "\n",
    "The complete code for the function to find all maximal cliques is then the following:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_all_maximal_cliques_bk(g, variant='degeneracy', print_result=False):\n",
    "    # Check that g is a NetworkX graph\n",
    "    if not isinstance(g, nx.classes.graph.Graph):\n",
    "        raise nx.NetworkXError('The provided graph is not a valid NetworkX undirected graph.')\n",
    "\n",
    "    # Classic Bron-Kerbosch algorithm\n",
    "    def bron_kerbosch(r, p, x):\n",
    "        if not p and not x:\n",
    "            if len(r) > 2:\n",
    "                yield r\n",
    "        for v in {*p}:\n",
    "            yield from bron_kerbosch(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "            p = p - {v}\n",
    "            x.add(v)\n",
    "\n",
    "    # Bron-Kerbosch algorithm with Tomita pivoting\n",
    "    def bron_kerbosch_tomita_pivot(r, p, x):\n",
    "        if not p and not x:\n",
    "            if len(r) > 2:\n",
    "                yield r\n",
    "        try:\n",
    "            u = max({(v, len({n for n in g.neighbors(v) if n in p})) for v in p | x}, key=lambda v: v[1])[0]\n",
    "            for v in p - {*g.neighbors(u)}:\n",
    "                yield from bron_kerbosch_tomita_pivot(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "                p = p - {v}\n",
    "                x.add(v)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Bron-Kerbosch algorithm with Tomita pivoting & degeneracy ordering\n",
    "    def bron_kerbosch_degeneracy(r, p, x):\n",
    "        for v in get_degeneracy_ordering(g):\n",
    "            yield from bron_kerbosch_tomita_pivot(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "            p = p - {v}\n",
    "            x.add(v)\n",
    "\n",
    "    # Main clique function\n",
    "    if g.nodes:\n",
    "        # Set initialization\n",
    "        r = {*()}\n",
    "        p = {*g.nodes}\n",
    "        x = {*()}\n",
    "\n",
    "        # Bron-Kerbosch algorithm\n",
    "        if variant == 'classic':\n",
    "            cliques = bron_kerbosch(r, p, x)\n",
    "        elif variant == 'tomita':\n",
    "            cliques = bron_kerbosch_tomita_pivot(r, p, x)\n",
    "        elif variant == 'degeneracy':\n",
    "            cliques = bron_kerbosch_degeneracy(r, p, x)\n",
    "        else:\n",
    "            warnings.warn('Invalid algorithm variant (\\'{}\\'). Using Bron-Kerbosch with degeneracy ordering as default.'.format(variant))\n",
    "            cliques = bron_kerbosch_degeneracy(r, p, x)\n",
    "\n",
    "        # Printing\n",
    "        if print_result:\n",
    "            print(*cliques, sep='\\n')\n",
    "\n",
    "        return cliques\n",
    "    else:\n",
    "        raise nx.NetworkXPointlessConcept('The provided graph is empty.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: the **degeneracy ordering function** has been left outside the `find_all_maximal_cliques()` method since it does not require particular checks on the graph (except those for its validity), nor the definition of subsets of nodes, meaning that unlike the Bron-Kerbosch algorithms, it can be used outside this particular application (for different purposes) without stringent requisites."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It should also be noted that sets $R$, $P$ and $X$ are implemented using Python's efficient [`set`](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset), and in particular empty sets are created using set literals `{*()}`\\[[10](https://www.python.org/dev/peps/pep-0448/)\\], which are slightly faster (and more elegant) than the equivalent `set()` constructor, as demonstrated by this code snippet:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Efficiency of set() vs. {*()}\n",
    "print('\\n' + 'Efficiency of {*()} vs. set():')\n",
    "\n",
    "number_set = 100000000\n",
    "empty_literal_time = (timeit.timeit('{*()}', number=number_set)) / number_set\n",
    "set_time = (timeit.timeit('set()', number=number_set)) / number_set\n",
    "\n",
    "print('- empty literal execution time: {} s.'.format(empty_literal_time))\n",
    "print('- set constructor execution time: {} s.'.format(set_time))\n",
    "if empty_literal_time < set_time:\n",
    "    print('Empty literal is faster than set constructor.')\n",
    "else:\n",
    "    print('Set constructor is faster than empty literal.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bron-Kerbosch algorithm results\n",
    "Let us now find all maximum cliques, and print them exactly once. Since our graph has more than 2000 nodes, this operation could take too long, so we will restrict this search to a **random subgraph of 100 vertices** in order to effectively test our algorithm while still saving time. NetworkX' library functions make the extraction of the subgraph immediate:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_random_subgraph(g, n):\n",
    "    # Check that g is a NetworkX graph\n",
    "    if not isinstance(g, nx.classes.graph.Graph):\n",
    "        raise nx.NetworkXError('The provided graph is not a valid NetworkX undirected graph.')\n",
    "\n",
    "    # Check that g is not empty\n",
    "    if g.nodes:\n",
    "        return g.subgraph(random.sample(g.nodes, n))\n",
    "    else:\n",
    "        raise nx.NetworkXPointlessConcept('The provided graph is empty.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting cliques, calculated using the efficient degeneracy ordering variant of the Bron-Kerbosch algorithm:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find and print all maximal cliques in a random subgraph of 100 nodes\n",
    "subgraph = sample_random_subgraph(g, 100)\n",
    "find_all_maximal_cliques_bk(subgraph, variant='degeneracy', print_result=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point we can **verify** the **efficiency** and **correctness** of the algorithm by running the other two Bron-Kerbosch variants and comparing their results:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Efficiency of different Bron-Kerbosch variants\n",
    "print('\\n' + 'Efficiency of different Bron-Kerbosch variants:')\n",
    "\n",
    "bk_classic_start = timeit.default_timer()\n",
    "bk_classic_cliques = find_all_maximal_cliques_bk(subgraph, variant='classic')\n",
    "bk_classic_end = timeit.default_timer()\n",
    "\n",
    "bk_tomita_start = timeit.default_timer()\n",
    "bk_tomita_cliques = find_all_maximal_cliques_bk(subgraph, variant='tomita')\n",
    "bk_tomita_end = timeit.default_timer()\n",
    "\n",
    "bk_degeneracy_start = timeit.default_timer()\n",
    "bk_degeneracy_cliques = find_all_maximal_cliques_bk(subgraph, variant='degeneracy')\n",
    "bk_degeneracy_end = timeit.default_timer()\n",
    "\n",
    "bk_classic_time = bk_classic_end - bk_classic_start\n",
    "bk_tomita_time = bk_tomita_end - bk_tomita_start\n",
    "bk_degeneracy_time = bk_degeneracy_end - bk_degeneracy_start\n",
    "\n",
    "print('- Bron-Kerbosch classic execution time: {} s.'.format(bk_classic_time))\n",
    "print('- Bron-Kerbosch with Tomita pivoting execution time: {} s.'.format(bk_tomita_time))\n",
    "print('- Bron-Kerbosch with degeneracy ordering execution time: {} s.'.format(bk_degeneracy_time))\n",
    "if bk_classic_time < bk_tomita_time and bk_classic_time < bk_degeneracy_time:\n",
    "    print('Bron-Kerbosch classic is faster than the other two variants.')\n",
    "elif bk_tomita_time < bk_classic_time and bk_tomita_time < bk_degeneracy_time:\n",
    "    print('Bron-Kerbosch with Tomita pivoting is faster than the other two variants.')\n",
    "elif bk_degeneracy_time < bk_classic_time and bk_degeneracy_time < bk_tomita_time:\n",
    "    print('Bron-Kerbosch with degeneracy ordering is faster than the other two variants.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Correctness of different Bron-Kerbosch variants\n",
    "print('\\n' + 'Checking the correctness of different Bron-Kerbosch variants... ', end='')\n",
    "\n",
    "bk_classic_cliques = list(bk_classic_cliques)\n",
    "bk_tomita_cliques = list(bk_tomita_cliques)\n",
    "bk_degeneracy_cliques = list(bk_degeneracy_cliques)\n",
    "\n",
    "correctness_flag = False\n",
    "\n",
    "if list(filter(lambda c: c not in bk_classic_cliques, bk_tomita_cliques)) or list(filter(lambda c: c not in bk_tomita_cliques, bk_classic_cliques)):\n",
    "    print('the cliques returned by the classic Bron-Kerbosch algorithm are different from those generated by the Tomita pivoting variant.')\n",
    "elif list(filter(lambda c: c not in bk_classic_cliques, bk_degeneracy_cliques)) or list(filter(lambda c: c not in bk_degeneracy_cliques, bk_classic_cliques)):\n",
    "    print('the cliques returned by the classic Bron-Kerbosch algorithm are different from those generated by the degeneracy ordering variant.')\n",
    "elif list(filter(lambda c: c not in bk_classic_cliques, bk_degeneracy_cliques)) or list(filter(lambda c: c not in bk_degeneracy_cliques, bk_classic_cliques)):\n",
    "    print('the cliques returned by the Tomita pivoting Bron-Kerbosch algorithm are different from those generated by the degeneracy ordering variant.')\n",
    "else:\n",
    "    correctness_flag = True\n",
    "    print('the cliques returned by all three algorithms are identical.')\n",
    "\n",
    "if correctness_flag:\n",
    "    print('All implemeneted variants are correct.')\n",
    "else:\n",
    "    print('There has been an error in the implementation of the Bron-Kerbosch algorithms.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Maximum clique\n",
    "At this point finding the **maximum clique** is as simple as getting the list of all cliques previously calculated and extract from it the **clique with the most elements**.\n",
    "\n",
    "The Python implementation below is an example of **overloaded function** which allows for greater flexibility in the choice of the input type. Indeed, its main argument `x` can either be:\n",
    "- a NetworkX undirected graph (in which case the `find_all_maximal_cliques_bk()` function is called in order to compute all cliques first), or\n",
    "- a list of all maximal cliques, stored as either lists or sets of nodes (which avoids re-computing all cliques if already present in some other variable)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_maximum_clique(x, print_result=False):\n",
    "    # Check input type\n",
    "    if isinstance(x, list):\n",
    "        cliques = x\n",
    "    else:\n",
    "        cliques = list(find_all_maximal_cliques_bk(x))\n",
    "\n",
    "    # Find maximum clique and convert to set (if input is a list of lists)\n",
    "    maximum_clique = set(cliques[np.argmax(np.array([len(c) for c in cliques]))])\n",
    "\n",
    "    # Printing\n",
    "    if print_result:\n",
    "        print(maximum_clique)\n",
    "\n",
    "    return maximum_clique"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this function is as simple as writing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find maximum clique\n",
    "maximum_clique = find_maximum_clique(bk_degeneracy_cliques, print_result=True)\n",
    "print('\\n' + 'The maximum clique of the random subgraph has length {} and contains nodes: \\n{}.'.format(len(maximum_clique), maximum_clique), end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions\n",
    "In this project we built a graph containing all authors extracted from all the comments of 83.218 OEIS JSON sequence files. We learnt about **regular expressions** in order to perform the parsing, and then delved into the variegated world of **cliques** to find one and all maximal cliques of the graph, as well as the largest one.\n",
    "\n",
    "We studied and correctly implemented **four different algorithms**, three of which as different ways to find all cliques, and introduced less popular (but not less important) concepts such as the **degeneracy** of a graph.\n",
    "\n",
    "In the end, we not only reached the main goal of the project by parsing the OEIS files and analyzing the graph, but also created a series of **well-readable and efficient Python implementations** of some significant algorithms, learning along the way about this language's best practices and data structures.\n",
    "\n",
    "The complete code for this project was originally written as a standalone module in the `mihalcea.py` Python file, which can be executed from the command line with an optional boolean argument, `--build_graph`, in order to either build the graph from scratch or load it from an existing JSON file. All functions described can be imported from said script in order to be used independently in other applications, and are fully documented in the `mihalcea.py` script, too."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "This project has been created and succesfully tested on the following machine:\n",
    "\n",
    "- **Motherboard:** MSI Nightblade X2\n",
    "- **CPU:** Intel Core i7-6700K @ 4.01 GHz, 8 core\n",
    "- **GPU:** AMD Radeon RX VEGA64 8GB\n",
    "- **RAM:** 16 GB DDR4 @ 2133 MHz\n",
    "- **SSD:** Samsung SSD 850 EVO 500 GB (540/520 MB/s r/w)\n",
    "- **HDD:** WD Blue 3 TB (7200 rpm, 180/220 MB/s r/w)\n",
    "- **OS:** Windows 10 Pro x64 1909\n",
    "- **IDE:** PyCharm Professional 2021.1\n",
    "- **Python:** 3.8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## License\n",
    "This work is licensed under a [Creative Commons ‚ÄúAttribution-NonCommercial-ShareAlike 4.0 International‚Äù](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en) license. More details are available in the [LICENSE](./LICENSE) file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\\[1\\] NetworkX, **Graph - Undirected graphs with self loops**, https://networkx.org/documentation/stable/reference/classes/graph.html\n",
    "\n",
    "\\[2\\] Guido van Rossum, Barry Warsaw, Nick Coghlan, **PEP 8 -- Style Guide for Python Code**, https://www.python.org/dev/peps/pep-0008/#function-and-variable-names\n",
    "\n",
    "\\[3\\] WolframMathWorld, **Clique**, https://mathworld.wolfram.com/Clique.html\n",
    "\n",
    "\\[4\\] WolframMathWorld, **Complete Graph**, https://mathworld.wolfram.com/CompleteGraph.html\n",
    "\n",
    "\\[5\\] WolframMathWorld, **Maximal Clique**, https://mathworld.wolfram.com/MaximalClique.html\n",
    "\n",
    "\\[6\\] Coen Bron, Joep Kerbosch, **Algorithm 457: finding all cliques of an undirected graph**, Communications of the ACM, vol. 16, issue 9 (Sept. 1973), https://dl.acm.org/doi/10.1145/362342.362367\n",
    "\n",
    "\\[7\\] Etsuji Tomita, Akira Tanaka, Haruhisa Takahashi, **The worst-case time complexity for generating all maximal cliques and computational experiments**, Theoretical Computer Science, 363 (1): 28‚Äì42, 2006, https://www.sciencedirect.com/science/article/pii/S0304397506003586\n",
    "\n",
    "\\[8\\] David Eppstein, Maarten L√∂ffler, Darren Strash, **Listing All Maximal Cliques in Sparse Graphs in Near-Optimal Time**, Algorithms and Computation, ISAAC 2010, Lecture Notes in Computer Science (vol 6506), Springer, https://link.springer.com/chapter/10.1007%2F978-3-642-17517-6_36\n",
    "\n",
    "\\[9\\] J. W. Moon, L. Moser, **On cliques in graphs**, Israel Journal of Mathematics, 3: 23‚Äì28, 1965, https://link.springer.com/article/10.1007/BF02760024\n",
    "\n",
    "\\[10\\] Jashua Landau, **PEP 448 -- Additional Unpacking Generalizations**, https://www.python.org/dev/peps/pep-0448/"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (OEIS-Comments-Authors-Max-Clique)",
   "language": "python",
   "name": "pycharm-29c0fc7f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}