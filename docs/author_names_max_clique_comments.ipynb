{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OEIS Analysis: Graph of Authors from Comments\n",
    "## Author: Paula Mihalcea\n",
    "#### Università degli Studi di Firenze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Requirements](#requirements)\n",
    "3. [Dataset](#dataset)\n",
    "4. [Author parsing](#author-parsing)\n",
    "5. [Graph building](#graph-building)\n",
    "6. [Maximal cliques](#maximal-cliques)\n",
    "    - [Definitions](#definitions)\n",
    "    - [The Bron-Kerbosch algorithm](#the-bron-kerbosch-algorithm)\n",
    "\t    - [Bron-Kerbosch with Tomita pivoting](#bron-kerbosch-with-tomita-pivoting)\n",
    "\t    - [Complexity](#complexity)\n",
    "- [Testing](#testing)\n",
    "- [License](#license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**[OEIS](https://oeis.org/)** is the online encyclopaedia of **integer sequences**. It lists *over 340.000* number sequences in lexicographic order, such as the [prime numbers](http://oeis.org/A000040) or the [Fibonacci sequence](http://oeis.org/A000045), easing the work of countless researchers since 1964, its foundation year.\n",
    "\n",
    "The OEIS is made of a series of **JSON files**, one for each integer sequence. Given their regular, human-readable format, these files can be easily manipulated in order to have many of their aspects further analyzed. Indeed, each page of the OEIS not only lists the integers of corresponding sequence, but also a series of information such as formulas, references, links and comments.\n",
    "\n",
    "This work aims to create, step-by-step, a **[Python 3](https://www.python.org/)** script capable of loading these files and parsing their content in order to build a **graph** where:\n",
    "- **nodes** represent all unique **authors** that can be found in each comment of every sequence;\n",
    "- **edges** link two authors who have **commented the same sequence**.\n",
    "\n",
    "Three algorithms are then implemented in order to find:\n",
    "- a maximal clique;\n",
    "- a list of all maximal cliques;\n",
    "- the maximum clique.\n",
    "\n",
    "The library of choice for creating the graph is **[NetworkX](https://networkx.org/)**, a fast Python module for the creation, manipulation, and study of the structure of complex networks. TODO Other data science packages such as [NumPy](https://numpy.org/) and [Matplotlib](https://matplotlib.org/) are also used for efficiency purposes, as they provide highly optimized functions specifically created for large datasets such as the OEIS encyclopaedia.\n",
    "TODO Other Python libraries such as [itertools](https://docs.python.org/3/library/itertools.html), [os](https://docs.python.org/3/library/os.html), [random](https://docs.python.org/3/library/random.html), and [sys](https://docs.python.org/3/library/sys.html) are also used for efficiency purposes, as they provide highly optimized functions specifically created for large datasets such as the OEIS encyclopaedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before starting, a series of packages must be installed in order for the subsequent code to be executable. The simplest way is to use [`pip`](https://pypi.org/project/pip/), a package manager for Python callable from the system terminal.\n",
    "\n",
    "The commands needed for this operation are listed in the following cell; the Jupyter magic function [`%%cmd`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-bash) (`%%bash` for Unix users) at the beginning allows to use it as a terminal. Make sure to follow the recommended install order, as it helps avoiding errors which can sometimes be generated by the different versions of the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%cmd\n",
    "\n",
    "pip install numpy\n",
    "pip install networkx\n",
    "pip install matplotlib\n",
    "pip install tqdm\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The freshly installed modules can be now used by simply importing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools as its\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import tqdm\n",
    "# TODO add other imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Having installed the required packages, we can now proceed with analyzing the dataset.\n",
    "\n",
    "The raw OEIS sequence files can be found in [`data/sequences`](./data/sequences/). We can start by writing a function capable of opening one of them using the [JSON package](https://docs.python.org/3/library/json.html) available in Python, and use it to load a file's content as a Python [dict](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict), then print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_json(file_path, print_content=False):\n",
    "    try:\n",
    "        file = open(file_path, 'r')\n",
    "    except OSError:\n",
    "        print('Could not open file:' + file + ', exiting program.')\n",
    "        sys.exit()\n",
    "    with file:\n",
    "        raw_data = json.load(file)\n",
    "        if print_content:\n",
    "            print('File ' + file_path.split('/')[-1] + ' contents:')\n",
    "            print()\n",
    "            print(json.dumps(raw_data, indent=True))\n",
    "            print()\n",
    "            print('The \\'json\\' Python module returns a dictionary, which can be confirmed by invoking the \\'type\\' function on the loaded data: ' + str(type(raw_data)) + '.')\n",
    "            print('This dictionary\\'s keys are: ' + str(raw_data.keys()).replace('dict_keys([', '').replace('])', '') + '.')\n",
    "        return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this function correctly **handles input/output errors**, and can be used to **return a file's content** as a Python **dictionary** even without printing it, by either omitting the `print_content` argument or setting it to `False` - a feature which will soon come in handy.\n",
    "\n",
    "We can thus view the first JSON file and its keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file = load_json('data/sequences/A000001.json', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, each sequence file contains additional information, specifically:\n",
    "- a simple `greeting`;\n",
    "- a `query`, containing the sequence's ID;\n",
    "- `count`;\n",
    "- `start`;\n",
    "- `results`, which contains a list with another dictionary as its first element.\n",
    "\n",
    "It can be seen from this file's content that the most relevant information is actually found in the **`results` sub-dictionary**, which can be easily accessed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results = file.get('results')\n",
    "\n",
    "if results:\n",
    "    print(json.dumps(results[0], indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are many different keys, among which we can find the one which is relevant to this project: the `comment` key containing a list of **comments** with their **authors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "comment_list = results[0].get('comment')\n",
    "\n",
    "if comment_list:\n",
    "    print(json.dumps(comment_list, indent=True))\n",
    "else:\n",
    "    print('No \"comments\" subsection found.' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author parsing\n",
    "\n",
    "Now that we know where to find the authors' names, we can proceed with building a function to parse all of them from a given file.\n",
    "\n",
    "### Regular expressions\n",
    "The most efficient way of doing this is to use a **regular expression** (also known as *regex*), a sequence of characters that specifies a *search pattern*\\[[1](https://en.wikipedia.org/wiki/Regular_expression)\\].\n",
    "\n",
    "We must first identify the ways in which the names have been written; by analyzing some comments, **six main patterns** have been identified, along with the **four regular expressions** needed to match them:\n",
    "1. *\"\\_Name Surname\\_\"* `(?<=_)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=_)`\n",
    "2. *\"\\[Name Surname\\]\"* and *\"\\[Surnamea, Surnameb\\]\"* `(?<=\\[)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=\\])`\n",
    "3. *\"- Name, Surname ( \"* and *\"\\- Name Surname, \"* `(?<=- )[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?= \\(|, )`\n",
    "4. *\"(Name Surname,\"* `(?<=\\()[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=,)`\n",
    "\n",
    "In spite of their apparent complexity, the meaning of these patterns is quite simple and be easily debugged with tools like [Regex101](https://regex101.com/). Each of them matches only strings that:\n",
    "- begin with certain characters `_`, `[`, `- `, `(`,\n",
    "    - followed by a capital letter `[A-Z]`,\n",
    "        - not followed by another capital letter `(?!=[A-Z])`,\n",
    "    - followed by at least any two characters `{2,}?`\n",
    "        - at the condition that none of them belong to a list of forbidden symbols `[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]` (where `^` is as a negation operator),\n",
    "- end with certain characters `_`, `]`, `(` or `, `, `,`.\n",
    "\n",
    "`(?>=)` and `(?=)` indicate that the matched strings should be preceded or followed (respectively) by the character(s) to the right of the `=` symbol.\n",
    "\n",
    "Escaping certain characters distinguishes them from a regex special symbol (e.g. `\\(\\)` matches the string *()*, while `()` is an empty regex group); whitespaces are simply represented by... a whitespace (` `).\n",
    "\n",
    "By combining these four expressions with the OR character (`|`) we can create the following regular expression to match all five patterns at once in Python:\n",
    "\n",
    "`(?<=_)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=_)|(?<=\\[)[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=\\])|(?<=- )[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?= \\(|, )|(?<=\\()[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?(?=,)`\n",
    "\n",
    "#### About this method's completeness\n",
    "It should be noted that these expressions do not find all the authors present in the comments because they are not written consistently across all sequences. One might argue that it would be sufficient finding all patterns used in order to get all the names; while this would be a good, if not really feasible solution (we do not know how many they are), the problem remains because certain patterns also match formulas and other unrelated data, making them unusable for retrieving only names.\n",
    "\n",
    "The definitive solution would be to either manually get the names, or to allow the matching of extraneous data in order to remove it later from the list of names; this way would take too long, though, and goes beyond the purpose of this project.\n",
    "\n",
    "### The parsing function\n",
    "The parsing function gets the **raw data** read by the JSON library in input and returns a **set of all author names** present in the comments of the loaded file (or `None` if there are none).\n",
    "\n",
    "Basically, after preparing the regex pattern (`re.compile()`), for each `comment` in the non-empty `comment_list` the function gets a list of the authors' names using Python's [`re`](https://docs.python.org/3/library/re.html) package for regular expressions, and uses it to update the set of unique authors called `authors` (which contains all names found in the file). The list comprehension in the `update` method is needed to flatten the many lists of lists returned by `re.findall()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_authors_from_comments(raw_data):\n",
    "    # Regex pattern\n",
    "    common_pattern = r'[A-Z](?!=[A-Z])[^0-9+\\(\\)\\[\\]\\{\\}\\\\\\/_:;\"\"]{2,}?'\n",
    "    pattern_list = [('(?<=_)', '(?=_)'), ('(?<=\\[)', '(?=\\])'), ('(?<=- )', '(?= \\(|, )'), ('(?<=\\()', '(?=,)')]\n",
    "    pattern = re.compile('|'.join([start + common_pattern + end for start, end in pattern_list]))\n",
    "\n",
    "    # Comment parsing\n",
    "    comment_list = raw_data.get('results')[0].get('comment')\n",
    "    if comment_list:\n",
    "        authors = set()\n",
    "        for comment in comment_list:\n",
    "            authors.update([n for names in re.findall(pattern, comment) for n in names.split(', ')])\n",
    "        return authors\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some observations:\n",
    "- the regex pattern is initially split into its **subpatterns** for better readability and to avoid repetitions;\n",
    "- this pattern has been accurately written so as to **not return empty matches**, normally generated by *capturing groups* (groups of characters between round parentheses) and for which additional `if`s would have been needed, resulting in a more complicated list comprehension;\n",
    "- **some sequences do not contain comments**, hence the check on `comment_list`;\n",
    "- a **set** has been chosen for the `authors_set` variable in order to **exclude duplicate names**, since the data needed for the project only concerns the presence or absence of a given author in the comments of a sequence, not all his/her instances. Python's [`set`](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset) data structure allows to store items in a hash table, without duplicating them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph building\n",
    "\n",
    "We can now proceed by parsing the authors from all OEIS sequences in the `data/sequences` directory and build their graph using the NetworkX library, eventually saving it to disk to avoid having to load every time all the JSON files.\n",
    "\n",
    "Considering that each **node** of the graph should contain the **name of a single author** (without duplicates), we only need to:\n",
    "1. Add each author of each sequence as a node;\n",
    "2. Add edges between all pairs of authors which have commented the same sequence.\n",
    "\n",
    "By repeating this procedure for every file in the `data/sequences` directory we get a graph of all authors, where people who have commented the same sequence are connected by an edge.\n",
    "\n",
    "The creation of such a graph is quite simple with the NetworkX library, since we only need to:\n",
    " - parse each sequence file;\n",
    " - extract its authors;\n",
    " - add them as nodes;\n",
    " - create a list of all possible pairs of authors in each sequence;\n",
    " - add an edge for each pair.\n",
    "\n",
    "Since the first two operations have been already implemented in the previous steps (see the `parse_authors_from_comments()` function), the other two are as simple as two lines of code, knowing that **NetworkX does not complain when adding existing nodes or edges**: we do not need to check every time if a given author has already been inserted or if a certain edge already exists, because the library will *not* duplicate them\\[[2](https://networkx.org/documentation/stable/reference/classes/graph.html)\\]. In fact, we could skip the `add_nodes_from()` function, since NetworkX automatically inserts non-existing nodes when adding edges connecting them.\n",
    "\n",
    "The best way to compute all author pairs for each sequence is given by the [itertools](https://docs.python.org/3/library/itertools.html) library, which implements efficient looping.\n",
    "\n",
    "Some notes about this function:\n",
    "- all it needs as input arguments is the **path** of the directory containing the JSON files and a **boolean flag** to specify if the resulting dataframe should also be saved to disk (instead of simply returned) - along with a name for the newly created JSON graph file, eventually (otherwise `comments_authors_graph.json` applies by default);\n",
    "- it begins with checking the correctness of the JSON files path and creating the necessary variables, among which:\n",
    "    - a list of all files in the given directory (using [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir));\n",
    "    - an empty NetwrokX graph `G`;\n",
    "    - a [tqdm](https://tqdm.github.io/) progress bar, only needed to visualize the overall progress of the parsing process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_graph_from_directory(dir_path, save=False, filename='comments_authors_graph'):\n",
    "    if dir_path[-1] != '/':\n",
    "        dir_path += '/'\n",
    "    file_list = [json_file for json_file in os.listdir(dir_path) if json_file.endswith('.json')]\n",
    "\n",
    "    # Prepare variables\n",
    "    g = nx.Graph()\n",
    "    progress_bar = tqdm.tqdm(total=len(file_list))\n",
    "\n",
    "    # Parse all JSON files\n",
    "    for f in file_list:\n",
    "        progress_bar.set_description('Parsing file {}'.format(f))\n",
    "        file_path = dir_path + f\n",
    "        raw_data = load_json(file_path)\n",
    "\n",
    "        authors = parse_authors_from_comments(raw_data)\n",
    "        if authors:\n",
    "            # g.add_nodes_from(authors)\n",
    "            g.add_edges_from(list(its.combinations(authors, 2)))\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Save graph\n",
    "    if save:\n",
    "        with open(dir_path.split('/')[0] + '/' + filename + '.json', 'w') as out_file:\n",
    "            json.dump(nx.readwrite.json_graph.node_link_data(g), out_file)\n",
    "\n",
    "    return g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph can thus be created by running:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = build_graph_from_directory('data/sequences', save=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and later retrieved by simply loading it with the function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_json_graph(file_path):\n",
    "    with open(file_path) as file:\n",
    "        return nx.readwrite.json_graph.node_link_graph(json.load(file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = load_json_graph('data/comments_authors_graph.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All variables names are lowercase with words separated by undescores in order to be compliant with the Python Enhancement Proposals 8 (PEP 8) style guide\\[[3](https://www.python.org/dev/peps/pep-0008/#function-and-variable-names)\\]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maximal cliques\n",
    "\n",
    "### Definitions\n",
    "Let $G = (\\mathcal{V}, \\mathcal{E})$ be an undirected graph where $\\mathcal{V}$ is the set of all nodes and $\\mathcal{E}$ the set of all edges.\n",
    "\n",
    "A **clique** of $G$ is a **complete subgraph**\\[[4](https://mathworld.wolfram.com/Clique.html)\\], or a simple undirected graph in which every pair of distinct vertices is connected by a unique edge\\[[5](https://en.wikipedia.org/wiki/Complete_graph)\\].\n",
    "\n",
    "A **maximal clique** is a clique that cannot be extended by including one more adjacent vertex, meaning it is not a subset of a larger clique. The **maximum clique** in a graph (i.e. the clique of largest size) is always maximal, while the converse does not hold\\[[6](https://mathworld.wolfram.com/MaximalClique.html)\\].\n",
    "\n",
    "### The Bron-Kerbosch algorithm\n",
    "In order to find one or all maximal cliques in our graph, as well as the maximum clique, we can proceed by implementing the **Bron-Kerbosch algorithm**\\[[7](https://dl.acm.org/doi/10.1145/362342.362367)\\], designed by its Dutch namesakes in 1973 and widely used in its variants for finding many types of communities (subsets of nodes more densely connected than the rest of the network) in graphs.\n",
    "\n",
    "This algorithm solves the following **core problem**\\[[8](https://e-l.unifi.it/course/view.php?id=20118)\\]:\n",
    "\n",
    "> Given three sets $R$, $P$ and $X$, find all maximal cliques that include:\n",
    "> - all of the vertices in $R$;\n",
    "> - some of the vertices in $P$;\n",
    "> - none of the vertices in $X$.\n",
    ">\n",
    "> Assuming that $P \\cap X = \\emptyset$, and for each $v \\in P \\cup X$, $R \\cup \\{v\\}$ is a clique, i.e. $v \\in \\mathcal{N}(R)$.\n",
    "\n",
    "To find all maximal cliques of graph $G$, we only need to initially set $R = \\emptyset$, $P = \\mathcal{V}$ and $X = \\emptyset$. After running the algorithm described in the pseudocode below, whenever $P = \\emptyset$ and $X = \\emptyset$ then there are no further elements that can be added to $R$, so $R$ is a maximal clique and the algorithm outputs it.\n",
    "\n",
    "```\n",
    "Bron-Kerbosch(R, P, X):\n",
    "    if P and X are both empty:\n",
    "        report R as a maximal clique\n",
    "    for each vertex v in P:\n",
    "        Bron-Kerbosch(R ⋃ {v}, P ⋂ N(v), X ⋂ N(v))\n",
    "        P = P \\ {v}\n",
    "        X = X ⋃ {v}\n",
    "```\n",
    "\n",
    "The set $X$ of vertices avoids listing cliques that have already been found, meaning that Bron-Kerbosch **lists each solution exactly once**.\n",
    "\n",
    "It should be noted, however, that often we lose time going in dead-ends, since every time $P = \\emptyset$ and $X = \\emptyset$ we back-track without producing anything.\n",
    "\n",
    "The following image illustrates how this algorithm works on a toy example.\n",
    "\n",
    "![](/bk_example.png)\n",
    "\n",
    "#### Bron-Kerbosch with Tomita pivoting\n",
    "The number of bad cases (where $P = \\emptyset$ and $X = \\emptyset$) can be reduced by choosing a pivot vertex $u$ from $P \\cup X$: any maximal clique must then include either $u$ or one of its non-neighbors, or otherwise the clique could be augmented by adding $u$ to it. Hence, only $u$ and its non-neighbors need to be tested as the choices for the vertex $v$ that is added to $R$ in each recursive call to the algorithm.\n",
    "\n",
    "A simple, effective way to choose the pivot is called the **Tomita pivoting**\\[[9](https://www.sciencedirect.com/science/article/pii/S0304397506003586)\\]:\n",
    "\n",
    "> The pivot $u \\in P \\cup X$ is the node having more neighbors in $P$\n",
    "\n",
    "```\n",
    "Bron-Kerbosch-Tomita-Pivoting(R, P, X):\n",
    "    if P and X are both empty:\n",
    "        report R as a maximal clique\n",
    "    choose pivot vertex u in P ⋃ X such that it is the node having more neighbors in P\n",
    "    for each vertex v in P \\ {N(u)}:\n",
    "        Bron-Kerbosch-Tomita-Pivoting(R ⋃ {v}, P ⋂ N(v), X ⋂ N(v))\n",
    "        P = P \\ {v}\n",
    "        X = X ⋃ {v}\n",
    "```\n",
    "\n",
    "#### Complexity\n",
    "The worst-case analysis for the Bron-Kerbosch algorithm with a pivot strategy matches the bound in \\[[10](https://link.springer.com/article/10.1007/BF02760024)\\], which proves that any $n$-vertex graph has at most $3^{\\frac{n}{3}}$ maximal cliques.\n",
    "\n",
    "Although other algorithms for solving the maximal clique problem yield better results on certain types of input, it has been frequently reported that in practice the Bron-Kerbosch algorithm, with its $O(3^{\\frac{n}{3}})$ **worst-case time complexity**, is more **efficient** than its alternatives.\n",
    "\n",
    "## Python maximal cliques algorithms\n",
    "As stated in the introduction, the aim of this project is to build **three algorithms** to find:\n",
    "1. a maximal clique;\n",
    "2. a list of all maximal cliques;\n",
    "3. the maximum clique.\n",
    "\n",
    "### Algorithm 1: finding a maximal clique\n",
    "The easiest way to perform this operation is to implement the efficient Bron-Kerbosch algorithm in such a way that it stops after having found a single maximal clique, which can be done with the following code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_a_maximal_clique(g, tomita=True, print_result=True):\n",
    "    if not isinstance(g, 'networkx.classes.graph.Graph'):\n",
    "        print('The provided graph is not a valid NetworkX undirected graph.')\n",
    "        return\n",
    "\n",
    "    def bron_kerbosch(r, p, x):\n",
    "        if not p and not x:\n",
    "            if len(r) > 2:\n",
    "                return r\n",
    "        else:\n",
    "            for v in {*p}:\n",
    "                return bron_kerbosch(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "\n",
    "    def bron_kerbosch_tomita_pivoting(r, p, x):\n",
    "        if not p and not x:\n",
    "            if len(r) > 2:\n",
    "                return r\n",
    "        else:\n",
    "            u = max({(v, len({n for n in g.neighbors(v) if n in p})) for v in p | x}, key=lambda v: v[1])[0]\n",
    "            for v in p - {*g.neighbors(u)}:\n",
    "                return bron_kerbosch_tomita_pivoting(r | {v}, p & {*g.neighbors(v)}, x & {*g.neighbors(v)})\n",
    "\n",
    "    if g.nodes:\n",
    "        # Initialization\n",
    "        r = {*()}\n",
    "        p = {*g.nodes}\n",
    "        x = {*()}\n",
    "\n",
    "        # Bron-Kerbosch algorithm\n",
    "        if tomita:\n",
    "            clique = bron_kerbosch_tomita_pivoting(r, p, x)\n",
    "        else:\n",
    "            clique = bron_kerbosch(r, p, x)\n",
    "\n",
    "        # Printing\n",
    "        if print_result:\n",
    "            print(clique)\n",
    "\n",
    "        return clique\n",
    "    else:\n",
    "        print('The provided graph is empty.')\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function `find_a_maximal_clique()` takes in input a NetworkX graph `g` and, optionally, two boolean flags for using the Tomita pivoting (`tomita`, `True` by default) and printing the clique found (`print_result`, also `True` by default).\n",
    "\n",
    "It checks whether the provided graph is a NetworkX undirected graph, and then proceeds with the definition of the two versions of the Bron-Kerbosch algorithm. It can be seen that they are a quite literal implementation of the pseudocode listed above, except that only non-trivial maximal cliques are returned, i.e. only those with more than 2 nodes are considered. The $R$, $P$ and $X$ sets are implemented using Python's efficient [`set`](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset), and in particular empty sets are created using set literals `{*()}`\\[[11](https://www.python.org/dev/peps/pep-0448/)\\], which apparently are faster than the equivalent `set()` constructor, as effectively demonstrated by this code snippet:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty literal execution time: 7.419999997182458e-08 s.\n",
      "Set constructor execution time: 1.5360000003283857e-07 s.\n"
     ]
    }
   ],
   "source": [
    "number = 1000\n",
    "print('Empty literal execution time: {} s.'.format((timeit.timeit('{*()}', number=number)) / number))\n",
    "print('Set constructor execution time: {} s.'.format((timeit.timeit('set()', number=number)) / number))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "This project has been created and succesfully tested on the following platform:\n",
    "\n",
    "- **Motherboard:** MSI Nightblade X2\n",
    "- **CPU:** Intel Core i7-6700K @ 4.01 GHz, 8 core\n",
    "- **GPU:** AMD Radeon RX VEGA64 8GB\n",
    "- **RAM:** 16 GB DDR4 @ 2133 MHz\n",
    "- **SSD:** Samsung SSD 850 EVO 500 GB (540/520 MB/s r/w)\n",
    "- **HDD:** WD Blue 3 TB (7200 rpm, 180/220 MB/s r/w)\n",
    "- **OS:** Windows 10 Pro x64 1909\n",
    "- **IDE:** PyCharm Professional 2021.1\n",
    "- **Python:** 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\\[1\\] Wikipedia, **Regular expression**, https://en.wikipedia.org/wiki/Regular_expression\n",
    "\n",
    "\\[2\\] NetworkX, **Graph - Undirected graphs with self loops**, https://networkx.org/documentation/stable/reference/classes/graph.html\n",
    "\n",
    "\\[3\\] Guido van Rossum, Barry Warsaw, Nick Coghlan, **PEP 8 -- Style Guide for Python Code**, https://www.python.org/dev/peps/pep-0008/#function-and-variable-names\n",
    "\n",
    "\\[4\\] WolframMathWorld, **Clique**, https://mathworld.wolfram.com/Clique.html\n",
    "\n",
    "\\[5\\] Wikipedia, **Complete graph**, https://en.wikipedia.org/wiki/Complete_graph\n",
    "\n",
    "\\[6\\] WolframMathWorld, **Maximal Clique**, https://mathworld.wolfram.com/MaximalClique.html\n",
    "\n",
    "\\[7\\] Coen Bron, Joep Kerbosch, **Algorithm 457: finding all cliques of an undirected graph**, Communications of the ACM, vol. 16, issue 9 (Sept. 1973), pp 575–577, https://dl.acm.org/doi/10.1145/362342.362367\n",
    "\n",
    "\\[8\\] Andrea Marino, **Finding Graph Patterns**, from the \"Advanced Algorithms and Graph Mining\" course at the Università degli Studi di Firenze, 2021, https://e-l.unifi.it/course/view.php?id=20118\n",
    "\n",
    "\\[9\\] Etsuji Tomita, Akira Tanaka, Haruhisa Takahashi, **The worst-case time complexity for generating all maximal cliques and computational experiments**, Theoretical Computer Science, 363 (1): 28–42, 2006, https://www.sciencedirect.com/science/article/pii/S0304397506003586\n",
    "\n",
    "\\[10\\] J. W. Moon, L. Moser, **On cliques in graphs**, Israel Journal of Mathematics, 3: 23–28, 1965, https://link.springer.com/article/10.1007/BF02760024\n",
    "\n",
    "\\[11\\] Jashua Landau, **PEP 448 -- Additional Unpacking Generalizations**, https://www.python.org/dev/peps/pep-0448/\n",
    "\n",
    "\\[12\\]\n",
    "\n",
    "\\[13\\]\n",
    "\n",
    "\\[14\\]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "This work is licensed under a [Creative Commons “Attribution-NonCommercial-ShareAlike 4.0 International”](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en) license. More details are available in the [LICENSE.md](./LICENSE.md) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-29c0fc7f",
   "language": "python",
   "display_name": "PyCharm (OEIS-Comments-Authors-Max-Clique)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}